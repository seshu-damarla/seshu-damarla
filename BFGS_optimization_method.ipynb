{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkgkDNCyKOhD4q/FRq/pbP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seshu-damarla/seshu-damarla/blob/main/BFGS_optimization_method.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "FqbxGBg4btWC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd7bad6d-c7c0-48dd-a04e-0fab0539186f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grad. func calculation\n",
            "optimal steplength calculation\n",
            "func calculation\n",
            "grad. func calculation\n",
            "func calculation\n",
            "grad. func calculation\n",
            "grad. func calculation\n",
            "iteration =  1\n",
            "B1 =  (2, 2)\n",
            "X1 =  [[0 0]]\n",
            "gradf1 =  [[ 1 -1]]\n",
            "S1 =  [[-1]\n",
            " [ 1]]\n",
            "lambda1 =  1.0\n",
            "k =  2\n",
            "X2 =  [[-1.  1.]]\n",
            "gradf2 =  [[-1. -1.]]\n",
            "norm(gradf) =  1.4142135623730951\n",
            "iteration =  2\n",
            "g1 =  [[-2.]\n",
            " [ 0.]]\n",
            "d1 =  [[-1.]\n",
            " [ 1.]]\n",
            "d1d1T =  [[ 1. -1.]\n",
            " [-1.  1.]]\n",
            "d1Tg1 =  [[2.]]\n",
            "d1g1T =  [[ 2.  0.]\n",
            " [-2.  0.]]\n",
            "g1d1T =  [[ 2. -2.]\n",
            " [ 0.  0.]]\n",
            "g1TB1g1 =  [[4.]]\n",
            "d1g1TB1 =  [[ 2.  0.]\n",
            " [-2.  0.]]\n",
            "B1g1d1T =  [[ 2. -2.]\n",
            " [ 0.  0.]]\n",
            "B2 =  [[ 0.5 -0.5]\n",
            " [-0.5  2.5]]\n",
            "S2 =  [[-0.]\n",
            " [ 2.]]\n",
            "optimal steplength calculation\n",
            "func calculation\n",
            "grad. func calculation\n",
            "func calculation\n",
            "grad. func calculation\n",
            "func calculation\n",
            "grad. func calculation\n",
            "func calculation\n",
            "grad. func calculation\n",
            "func calculation\n",
            "grad. func calculation\n",
            "func calculation\n",
            "grad. func calculation\n",
            "func calculation\n",
            "grad. func calculation\n",
            "func calculation\n",
            "grad. func calculation\n",
            "lambda2 =  0.015625\n",
            "k =  5\n",
            "X3 =  [[-1.       1.03125]]\n",
            "grad. func calculation\n",
            "gradf3 =  [[-0.9375 -0.9375]]\n",
            "norm(gradf) =  1.3258252147247767\n",
            "iteration =  3\n",
            "g1 =  [[0.0625]\n",
            " [0.0625]]\n",
            "d1 =  [[-0.     ]\n",
            " [ 0.03125]]\n",
            "d1d1T =  [[0.         0.        ]\n",
            " [0.         0.00097656]]\n",
            "d1Tg1 =  [[0.00195312]]\n",
            "d1g1T =  [[0.         0.        ]\n",
            " [0.00195312 0.00195312]]\n",
            "g1d1T =  [[0.         0.00195312]\n",
            " [0.         0.00195312]]\n",
            "g1TB1g1 =  [[0.0078125]]\n",
            "d1g1TB1 =  [[0.         0.        ]\n",
            " [0.         0.00390625]]\n",
            "B1g1d1T =  [[0.         0.        ]\n",
            " [0.         0.00390625]]\n",
            "B2 =  [[ 0.5 -0.5]\n",
            " [-0.5  1. ]]\n",
            "S2 =  [[-0.     ]\n",
            " [ 0.46875]]\n",
            "optimal steplength calculation\n",
            "func calculation\n",
            "grad. func calculation\n",
            "func calculation\n",
            "grad. func calculation\n",
            "lambda2 =  1.0\n",
            "k =  2\n",
            "X3 =  [[-1.   1.5]]\n",
            "grad. func calculation\n",
            "gradf3 =  [[0. 0.]]\n",
            "norm(gradf) =  0.0\n",
            "optimum =  [[-1.   1.5]]\n",
            "norm(gradf) =  0.0\n"
          ]
        }
      ],
      "source": [
        "# BROYDEN–FLETCHER–GOLDFARB–SHANNO METHOD\n",
        "# Unconstrained nonlinear multivariable optimization\n",
        "# By Seshu Damarla, Date: 11 Sep. 2024\n",
        "# Engineering Optimization:Theory and Practice by Singiresu S. Rao, JOHN WILEY & SONS, INC.\n",
        "# minimize f(x1,x2) = x1 - x2 + 2*(x1^2) + 2*x1*x2 + (x2^2)\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# objective function\n",
        "def func(X):\n",
        "  print('func calculation')\n",
        "  x1 = X[0,0]\n",
        "  x2 = X[0,1]\n",
        "  #print('x1 = ',x1)\n",
        "  #print('x2 = ',x2)\n",
        "  return x1 - x2 + 2*(x1**2) + 2*x1*x2 + x2**2\n",
        "\n",
        "# gradient of objective function\n",
        "def grad_func(X):\n",
        "  print('grad. func calculation')\n",
        "  x1 = X[0,0]\n",
        "  x2 = X[0,1]\n",
        "  grad = np.array([[1 + 4*x1 + 2*x2, -1 + 2*x1 + 2*x2]])\n",
        "  return grad\n",
        "\n",
        "# line search algorith to determine optimal stpe length at each iteration\n",
        "def optimal_steplength(X,S):\n",
        "  print('optimal steplength calculation')\n",
        "  alpha = 1\n",
        "  c1 = (0.001)/2\n",
        "  tau = 0.5\n",
        "  #print('X = ',X)\n",
        "  #print('S = ',S)\n",
        "  #print('alpha = ',alpha)\n",
        "  #print('c1 = ',c1)\n",
        "  #print('tau = ',tau)\n",
        "  #XX = np.array(XX)\n",
        "  k = 1;\n",
        "  phi_alpha = 100\n",
        "  phi_zero = 1\n",
        "  phi_zero1 = 1\n",
        "  while phi_alpha > (phi_zero + c1*alpha*phi_zero1): #func(X) + c1*alpha*np.dot(grad_func(X),S):\n",
        "    XX = X + alpha*S.T\n",
        "    XX = XX.reshape(1,2)\n",
        "    #print('XX =',XX)\n",
        "    phi_alpha = func(XX)\n",
        "    phi_alpha1 = np.dot(grad_func(X + alpha*S.T),S)\n",
        "    phi_zero = func(X)\n",
        "    phi_zero1 = np.dot(grad_func(X),S)\n",
        "    alpha = (tau**(k-1))*alpha\n",
        "    k = k+1\n",
        "    #rint('phi_alpha = ',phi_alpha)\n",
        "    #print('phi_alpha1 = ',phi_alpha1)\n",
        "    #print('phi_zero = ',phi_zero)\n",
        "    #print('phi_zero1 = ',phi_zero1)\n",
        "  return alpha,k\n",
        "\n",
        "#norm_gradf = 1000\n",
        "epsilon = 0.0001\n",
        "X1 = np.array([[0,0]])  # initial point\n",
        "B1 = np.array([[1,0],[0,1]])  # initial aapproximaiton of the inverse of Hessian matrix\n",
        "i = 1\n",
        "norm_gradf = 1000\n",
        "\n",
        "# gradf1 = grad_func(X1)\n",
        "# S1 = -B1*gradf1'\n",
        "# S1 = -np.dot(B1,gradf.T)\n",
        "# lambda1 = optimal_steplength(X1,S1)\n",
        "# X2 = X1 + lambda1*S1\n",
        "# norm_gradf = np.linalg.norm(grad_func(X2))\n",
        "\n",
        "while norm_gradf > epsilon:\n",
        "  if i == 1:\n",
        "    gradf1 = grad_func(X1).reshape(1,2)\n",
        "    S1 = -np.dot(B1,gradf1.T).reshape(2,1)\n",
        "    lambda1,k = optimal_steplength(X1,S1)\n",
        "    X2 = X1 + lambda1*S1.T\n",
        "    gradf2 = grad_func(X2)\n",
        "    norm_gradf = np.linalg.norm(gradf2)\n",
        "    print('iteration = ',i)\n",
        "    print('B1 = ',B1.shape)\n",
        "    print('X1 = ',X1)\n",
        "    print('gradf1 = ',gradf1)\n",
        "    print('S1 = ',S1)\n",
        "    print('lambda1 = ',lambda1)\n",
        "    print('k = ',k)\n",
        "    print('X2 = ',X2)\n",
        "    print('gradf2 = ',gradf2)\n",
        "    print('norm(gradf) = ',norm_gradf)\n",
        "  elif i > 1:\n",
        "    print('iteration = ',i)\n",
        "    g1 = gradf2.T - gradf1.T\n",
        "    print('g1 = ',g1)\n",
        "    d1 = lambda1 * S1\n",
        "    print('d1 = ',d1)\n",
        "    d1d1T = np.dot(d1,d1.T)\n",
        "    print('d1d1T = ',d1d1T)\n",
        "    d1Tg1 = np.dot(d1.T,g1)\n",
        "    print('d1Tg1 = ',d1Tg1)\n",
        "    d1g1T = np.dot(d1,g1.T)\n",
        "    print('d1g1T = ',d1g1T)\n",
        "    g1d1T = np.dot(g1,d1.T)\n",
        "    print('g1d1T = ',g1d1T)\n",
        "    g1TB1g1 = np.dot(g1.T,np.dot(B1,g1))\n",
        "    print('g1TB1g1 = ',g1TB1g1)\n",
        "    d1g1TB1 = np.dot(d1,np.dot(g1.T,B1))\n",
        "    print('d1g1TB1 = ',d1g1TB1)\n",
        "    B1g1d1T = np.dot(B1,g1d1T)\n",
        "    print('B1g1d1T = ',B1g1d1T)\n",
        "    B2 = B1 + (1+(g1TB1g1/d1Tg1))*(d1d1T/d1Tg1) - (d1g1TB1/d1Tg1) - (B1g1d1T/d1Tg1)\n",
        "    print('B2 = ',B2)\n",
        "    S2 = -np.dot(B2,gradf2.T)\n",
        "    print('S2 = ',S2)\n",
        "    lambda2,k = optimal_steplength(X2,S2)\n",
        "    print('lambda2 = ',lambda2)\n",
        "    print('k = ',k)\n",
        "    X3 = X2 + lambda2*S2.T\n",
        "    print('X3 = ',X3)\n",
        "    gradf3 = grad_func(X3)\n",
        "    print('gradf3 = ',gradf3)\n",
        "    norm_gradf = np.linalg.norm(gradf3)\n",
        "    print('norm(gradf) = ',norm_gradf)\n",
        "    X1 = X2\n",
        "    gradf1 = gradf2\n",
        "    S1 = S2\n",
        "    lambda1 = lambda2\n",
        "    X2 = X3\n",
        "    gradf2 = gradf3\n",
        "    B1 = B2\n",
        "\n",
        "  i=i+1\n",
        "\n",
        "print('optimum = ',X2)\n",
        "print('norm(gradf) = ',norm_gradf)\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}